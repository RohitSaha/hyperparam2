{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Approach to Bayesian Optimization (GPyOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization is a way to try and find a maximum or minimum (global or local) of an unknown function. In our lab, these functions are models that are time intensive and expensive to evaluate, so we want to do this optimization in as few evaluations as possible. \n",
    "\n",
    "Right now, to illustrate a basic bayesian optimization routine, we will try and minimize a much simpler function that we understand very well. The function is\n",
    "\n",
    "$$f(x) = (x-3)*(x-2)*(x-1)*x*(x+2)*(x+3)$$\n",
    "\n",
    "which has a global min at -2.6 and two other local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "f = lambda x: (x-3)*(x-2)*(x-1)*x*(x+2)*(x+3)\n",
    "\n",
    "plt.plot((np.array(range(620))-310)/100.,list(map(f, (np.array(range(620))-310)/100.)))\n",
    "plt.plot((-3.1,3.1),(0,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the actual optimization, we will first use an open source package called [GPyOpt](http://sheffieldml.github.io/GPyOpt/z) from the University of Sheffield. This is based on another package produced by the same group called [GPy](https://sheffieldml.github.io/GPy/) which is used for general gaussian processes. Below, we import the BayesianOptimization method from the GPyOpt package, and define our function.\n",
    "\n",
    "We then evaluate `foo` at three different points to have some history that we can pass into our optimization routine. Inputs into the Bayestian Optimization function must be in 2D numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from GPyOpt.methods import BayesianOptimization\n",
    "import numpy as np\n",
    "\n",
    "def foo(x):\n",
    "    return (x-3)*(x-2)*(x-1)*x*(x+2)*(x+3)\n",
    "\n",
    "x_init = np.array([[2],[0.5],[-2.9]]) \n",
    "y_init = foo(x_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass our initial $x$ and $y$ values into the BayesianOptimization routine, along with the limits we will set for possible $x$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBopt = BayesianOptimization(f=None, \n",
    "                              X = x_init, \n",
    "                              Y = y_init, \n",
    "                              domain=[{'name':'x','type': 'continuous','domain': (-3.3,3.3)}])\n",
    "print \"The next suggested sample is: \" + str(float(myBopt.suggested_sample[[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is found by using the acquisition function, which balances spatial exploration with potential for improvement. Below, we can plot the acquisition function given our 3 function evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBopt.plot_acquisition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, we can get insight about the distribution imposed over all functions in $\\mathbb{R}^2$ by the Gaussian Process. There are two plots here, the first of the distribution given by the Gaussian Process (the black line surrounded by the blue confidence intervals) and the acquisition function (the red line at the bottom). Our suggested sample is given by the maximum of the acquisition function.\n",
    "\n",
    "(If we had initialized our Gaussian Process on 3 different points, we can observe the new distribution and acquisition function below:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init1 = np.array([[-1.5],[0.7],[1.8]])\n",
    "y_init1 = foo(x_init1)\n",
    "myBopt1 = BayesianOptimization(f=None, X = x_init1, Y = y_init1, bounds=[(-3.3,3.3)])\n",
    "myBopt1.plot_acquisition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we generate more function evaluations, we have a better and better understanding of the underlying function and the distribution of functions gets more concentrated around the \"mean\" function (the black line in the above plots). Let's generate a few more samples and observe what happens to our acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBopt.plot_acquisition()\n",
    "print \"The last suggested sample was: \" + str(float(myBopt.suggested_sample[[0]]))\n",
    "\n",
    "#Stack the suggested sample onto our function history\n",
    "x_init = np.vstack([x_init,myBopt.suggested_sample])\n",
    "y_init = np.vstack([y_init,foo(myBopt.suggested_sample)])\n",
    "\n",
    "#Generate the next new sample\n",
    "myBopt = BayesianOptimization(f=None, X = x_init, Y = y_init, bounds=[(-3.3,3.3)])\n",
    "myBopt.plot_acquisition()\n",
    "print \"The next suggested sample is: \" + str(float(myBopt.suggested_sample[[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
